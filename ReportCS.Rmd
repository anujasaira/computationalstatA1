---
title: "Report"
author: "Aldo Giovanni e Giacomo"
date: "2023-05-25"
output:
  pdf_document: default
  html_document: default
editor_options:
  markdown:
    wrap: sentence
---


**INTRODUCTION**

Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor.
The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class.
The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets.
The vital element is the instability of the prediction method.
If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.
$\(\ell\)$
A learning set $\mathcal{L}$ consists of data {($\mathit{y_n}$, $\mathit{x_n}$),$\mathit{n}$ = 1....
, $\mathit{N}$}

$\mathit{y}$ : class labels/numeric responses

$\mathit{x}$ : input

$\varphi(\mathit{x},\mathcal{L})$ : predictor

$\mathcal{L_k}$ : sequence of learning sets consisting of N independent observations from the same underlying distribution as $\mathcal{L}$.

Our scope of this project is to show that the sequence of predictors $\varphi(\mathit{x},\mathcal{L_k})$ is a better predictor than $\varphi(\mathit{x},\mathcal{L})$.

-   $\mathit{y}$ is numeric

    Replace $\varphi(\mathit{x},\mathcal{L})$ by the average of $\varphi(\mathit{x},\mathcal{L_k})$.
    In other words find the expecation of the predictor over $\mathcal{L}$ .

-   $\mathit{y}$ is class labels

-   
    			

    		

    	

Assume we have a procedure for using this learning set to form a predictor 9v(x, £) -- if the input is x we predict y by \~d(x, £).
Now, suppose we are given a sequence of learnings sets {£k } each consisting of N independent observations from the same underlying distribution as £.
Our mission is to use the {£\~} to get a better predictor than the single learning set predictor 9:(x,/2).
The restriction is that all we are allowed to work with is the sequence of predictors {\~(x, £k)}.
If y is numerical, an obvious procedure is to replace c;(x, £) by the average of 97(x, £k) over k, i.e. by 9;A(X) -- E£9:(x, £) where E£ denotes the expectation over £, and the subscript A in C;A denotes aggregation.
If 9\~(x, £) predicts a class j E {1,..., J}, then one method of aggregating the \~y(x, Z\~) is by voting.
Let "\\4 = nr{k; \~(x, £#) = j} and take 9\~A(X) = argmaxjNj, that is, the j for which \~,\~-is maximum. \## Bagging Classification Trees

In a classification problem in which Y is qualitative, there are a few possible approaches, but the simplest is as follows.
For a given test observation, we can record the class predicted by each of the B trees, and take a majority vote: the overall prediction is the most commonly occurring class among the B predictions.

We are going to see results for moderate sized data sets.
In all runs the following procedure was used: i) The data set is randomly divided into a test set T and a learning set L.
In the real data sets T is 10% of the data.
ii) A classification tree is constructed from L using 10-fold cross-validation.
Running the test set T down this tree gives the misclassification rate es(L, T).
iii) A bootstrap sample Lb is selected from L, and a tree grown using Lb.
The original learning set L is used as test set to select the best pruned subtree(but how?)##############################################################ASK##############################################################################################################################################################################.This is repeated 50 times.
iv) If (jn, xn) belong to T, then the estimated class of Xn is that class having the plurality.
(If there is a tie, the estimated class is the one with the lowest class label.) The proportion of times the estimated class differs from the true class is the bagging misclassification rate eb(L, T).
v) The random division of the data into L and T is repeated 100 times and the reported es(L,T) and eb(L,T) are the averages over the 100 iterations.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(randomForest)

```

We are going to use diabetes dataset compare the result of decision tree and random forest.
Random forest is also known as bagged decision tree.
The random forest is a classification algorithm consisting of many decisions trees.
It uses bagging and feature randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual tree.

```{r}


misclassification_rate_vec <- rep(0,100)
misclassification_rate_vec_rf <- rep(0,100)

E_sim <- read.table('/Users/anujaabraaham/Downloads/diabetes.csv',sep = ',',header = TRUE)
#E_sim <- diabetes
E_sim$Outcome<- as.factor(E_sim$Outcome)

head(E_sim)
```

This is a data base gathered among the Pima Indians by the National Institute of Diabetes and Digestive and Kidney Diseases.
The data base consists of 768 cases, 8 variables and two classes.
The variables are medical measurements on the patient plus age and pregnancy information.
The classes are: tested positive for diabetes (268) or negative (500).

As per how Breiman did the analysis we followed a similar pattern with raw data instead of simulated data.
The data is split into 2 a learning set, L, (10% of the data) with 76 rows and a test set, T, (90% of the data) with 692 rows.
Next we will train the decision tree model using train() function with 10-fold cross-validation.
A bagged decision tree aka random forest model is created in parallel using the function randomForest().
The two models are made to predict the result using the same dataset and the error is calculated.
The whole process is iterated 100 times for different combination of learning and test data and the mean error is calculated.

```{r}
for (i in 1:100){
  index <- createDataPartition(E_sim$Outcome, p = 0.9, list = FALSE)

  # Divide the data into test set T and learning set L
  L <- E_sim[-index, ]
  T <- E_sim[index, ]
  
  # Train the classification tree model with 10-fold cross-validation 
  modeldt <- train(
    Outcome ~ ., 
    data = L, 
    method = "rpart", 
    trControl = trainControl(method = "cv", number = 10)
  )
  
  rf_model <- randomForest(Outcome ~ ., data = L, proximity=TRUE)

  

  # Predict the class labels for the test set using the trained model
  predictions <- predict(modeldt, newdata = T)

  # Calculate misclassification rate
  misclassification_rate <- mean(predictions != T$Outcome)

  misclassification_rate_vec[i] <- misclassification_rate
  
  
  predictions_rf <- predict(rf_model, newdata = T)

  # Calculate misclassification rate
  misclassification_rate_rf <- mean(predictions_rf != T$Outcome)

  misclassification_rate_vec_rf[i] <- misclassification_rate_rf
  
}



```

```{r}
mis<- mean(misclassification_rate_vec)
mis_rf <- mean(misclassification_rate_vec_rf)
mis
mis_rf
```

From the result it is clear that the model using bagged predictor is better than the model without the bagged predictor.
Bagged predictor works better for unstable classifiers than for stable ones.
