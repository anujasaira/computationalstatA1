% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Report},
  pdfauthor={Aldo Giovanni e Giacomo},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Report}
\author{Aldo Giovanni e Giacomo}
\date{2023-05-25}

\begin{document}
\maketitle

\textbf{INTRODUCTION}

Bagging predictors is a method for generating multiple versions of a
predictor and using these to get an aggregated predictor. The
aggregation averages over the versions when predicting a numerical
outcome and does a plurality vote when predicting a class. The multiple
versions are formed by making bootstrap replicates of the learning set
and using these as new learning sets. The vital element is the
instability of the prediction method. If perturbing the learning set can
cause significant changes in the predictor constructed, then bagging can
improve accuracy. \(\(\ell\)\) A learning set \(\mathcal{L}\) consists
of data \{(\(\mathit{y_n}\), \(\mathit{x_n}\)),\(\mathit{n}\) = 1\ldots.
, \(\mathit{N}\)\}

\(\mathit{y}\) : class labels/numeric responses

\(\mathit{x}\) : input

\(\varphi(\mathit{x},\mathcal{L})\) : predictor

\(\mathcal{L_k}\) : sequence of learning sets consisting of N
independent observations from the same underlying distribution as
\(\mathcal{L}\).

Our scope of this project is to show that the sequence of predictors
\(\varphi(\mathit{x},\mathcal{L_k})\) is a better predictor than
\(\varphi(\mathit{x},\mathcal{L})\).

\begin{itemize}
\item
  \(\mathit{y}\) is numeric

  Replace \(\varphi(\mathit{x},\mathcal{L})\) by the average of
  \(\varphi(\mathit{x},\mathcal{L_k})\). In other words find the
  expecation of the predictor over \(\mathcal{L}\) .
\item
  \(\mathit{y}\) is class labels
\item
\end{itemize}

Assume we have a procedure for using this learning set to form a
predictor 9v(x, £) -- if the input is x we predict y by
\textasciitilde d(x, £). Now, suppose we are given a sequence of
learnings sets \{£k \} each consisting of N independent observations
from the same underlying distribution as £. Our mission is to use the
\{£\textasciitilde\} to get a better predictor than the single learning
set predictor 9:(x,/2). The restriction is that all we are allowed to
work with is the sequence of predictors \{\textasciitilde(x, £k)\}. If y
is numerical, an obvious procedure is to replace c;(x, £) by the average
of 97(x, £k) over k, i.e.~by 9;A(X) -- E£9:(x, £) where E£ denotes the
expectation over £, and the subscript A in C;A denotes aggregation. If
9\textasciitilde(x, £) predicts a class j E \{1,\ldots, J\}, then one
method of aggregating the \textasciitilde y(x, Z\textasciitilde) is by
voting. Let ``\textbackslash4 = nr\{k; \textasciitilde(x, £\#) = j\} and
take 9\textasciitilde A(X) = argmaxjNj, that is, the j for which
\textasciitilde,\textasciitilde-is maximum. \#\# Bagging Classification
Trees

In a classification problem in which Y is qualitative, there are a few
possible approaches, but the simplest is as follows. For a given test
observation, we can record the class predicted by each of the B trees,
and take a majority vote: the overall prediction is the most commonly
occurring class among the B predictions.

We are going to see results for moderate sized data sets. In all runs
the following procedure was used: i) The data set is randomly divided
into a test set T and a learning set L. In the real data sets T is 10\%
of the data. ii) A classification tree is constructed from L using
10-fold cross-validation. Running the test set T down this tree gives
the misclassification rate es(L, T). iii) A bootstrap sample Lb is
selected from L, and a tree grown using Lb. The original learning set L
is used as test set to select the best pruned subtree(but
how?)\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#ASK\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#\#.This
is repeated 50 times. iv) If (jn, xn) belong to T, then the estimated
class of Xn is that class having the plurality. (If there is a tie, the
estimated class is the one with the lowest class label.) The proportion
of times the estimated class differs from the true class is the bagging
misclassification rate eb(L, T). v) The random division of the data into
L and T is repeated 100 times and the reported es(L,T) and eb(L,T) are
the averages over the 100 iterations.

We are going to use diabetes dataset compare the result of decision tree
and random forest. Random forest is also known as bagged decision tree.
The random forest is a classification algorithm consisting of many
decisions trees. It uses bagging and feature randomness when building
each individual tree to try to create an uncorrelated forest of trees
whose prediction by committee is more accurate than that of any
individual tree.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{misclassification\_rate\_vec }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)}
\NormalTok{misclassification\_rate\_vec\_rf }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{100}\NormalTok{)}

\NormalTok{E\_sim }\OtherTok{\textless{}{-}} \FunctionTok{read.table}\NormalTok{(}\StringTok{\textquotesingle{}/Users/anujaabraaham/Downloads/diabetes.csv\textquotesingle{}}\NormalTok{,}\AttributeTok{sep =} \StringTok{\textquotesingle{},\textquotesingle{}}\NormalTok{,}\AttributeTok{header =} \ConstantTok{TRUE}\NormalTok{)}
\CommentTok{\#E\_sim \textless{}{-} diabetes}
\NormalTok{E\_sim}\SpecialCharTok{$}\NormalTok{Outcome}\OtherTok{\textless{}{-}} \FunctionTok{as.factor}\NormalTok{(E\_sim}\SpecialCharTok{$}\NormalTok{Outcome)}

\FunctionTok{head}\NormalTok{(E\_sim)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Pregnancies Glucose BloodPressure SkinThickness Insulin  BMI
## 1           6     148            72            35       0 33.6
## 2           1      85            66            29       0 26.6
## 3           8     183            64             0       0 23.3
## 4           1      89            66            23      94 28.1
## 5           0     137            40            35     168 43.1
## 6           5     116            74             0       0 25.6
##   DiabetesPedigreeFunction Age Outcome
## 1                    0.627  50       1
## 2                    0.351  31       0
## 3                    0.672  32       1
## 4                    0.167  21       0
## 5                    2.288  33       1
## 6                    0.201  30       0
\end{verbatim}

This is a data base gathered among the Pima Indians by the National
Institute of Diabetes and Digestive and Kidney Diseases. The data base
consists of 768 cases, 8 variables and two classes. The variables are
medical measurements on the patient plus age and pregnancy information.
The classes are: tested positive for diabetes (268) or negative (500).

As per how Breiman did the analysis we followed a similar pattern with
raw data instead of simulated data. The data is split into 2 a learning
set, L, (10\% of the data) with 76 rows and a test set, T, (90\% of the
data) with 692 rows. Next we will train the decision tree model using
train() function with 10-fold cross-validation. A bagged decision tree
aka random forest model is created in parallel using the function
randomForest(). The two models are made to predict the result using the
same dataset and the error is calculated. The whole process is iterated
100 times for different combination of learning and test data and the
mean error is calculated.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{100}\NormalTok{)\{}
\NormalTok{  index }\OtherTok{\textless{}{-}} \FunctionTok{createDataPartition}\NormalTok{(E\_sim}\SpecialCharTok{$}\NormalTok{Outcome, }\AttributeTok{p =} \FloatTok{0.9}\NormalTok{, }\AttributeTok{list =} \ConstantTok{FALSE}\NormalTok{)}

  \CommentTok{\# Divide the data into test set T and learning set L}
\NormalTok{  L }\OtherTok{\textless{}{-}}\NormalTok{ E\_sim[}\SpecialCharTok{{-}}\NormalTok{index, ]}
\NormalTok{  T }\OtherTok{\textless{}{-}}\NormalTok{ E\_sim[index, ]}
  
  \CommentTok{\# Train the classification tree model with 10{-}fold cross{-}validation }
\NormalTok{  modeldt }\OtherTok{\textless{}{-}} \FunctionTok{train}\NormalTok{(}
\NormalTok{    Outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }
    \AttributeTok{data =}\NormalTok{ L, }
    \AttributeTok{method =} \StringTok{"rpart"}\NormalTok{, }
    \AttributeTok{trControl =} \FunctionTok{trainControl}\NormalTok{(}\AttributeTok{method =} \StringTok{"cv"}\NormalTok{, }\AttributeTok{number =} \DecValTok{10}\NormalTok{)}
\NormalTok{  )}
  
\NormalTok{  rf\_model }\OtherTok{\textless{}{-}} \FunctionTok{randomForest}\NormalTok{(Outcome }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ L, }\AttributeTok{proximity=}\ConstantTok{TRUE}\NormalTok{)}

  

  \CommentTok{\# Predict the class labels for the test set using the trained model}
\NormalTok{  predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(modeldt, }\AttributeTok{newdata =}\NormalTok{ T)}

  \CommentTok{\# Calculate misclassification rate}
\NormalTok{  misclassification\_rate }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(predictions }\SpecialCharTok{!=}\NormalTok{ T}\SpecialCharTok{$}\NormalTok{Outcome)}

\NormalTok{  misclassification\_rate\_vec[i] }\OtherTok{\textless{}{-}}\NormalTok{ misclassification\_rate}
  
  
\NormalTok{  predictions\_rf }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(rf\_model, }\AttributeTok{newdata =}\NormalTok{ T)}

  \CommentTok{\# Calculate misclassification rate}
\NormalTok{  misclassification\_rate\_rf }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(predictions\_rf }\SpecialCharTok{!=}\NormalTok{ T}\SpecialCharTok{$}\NormalTok{Outcome)}

\NormalTok{  misclassification\_rate\_vec\_rf[i] }\OtherTok{\textless{}{-}}\NormalTok{ misclassification\_rate\_rf}
  
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mis}\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(misclassification\_rate\_vec)}
\NormalTok{mis\_rf }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(misclassification\_rate\_vec\_rf)}
\NormalTok{mis}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.298815
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mis\_rf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2601734
\end{verbatim}

From the result it is clear that the model using bagged predictor is
better than the model without the bagged predictor. Bagged predictor
works better for unstable classifiers than for stable ones.

\end{document}
