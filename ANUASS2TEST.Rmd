---
title: "Assignment II"
author: "Anuja Saira Abraham"
date: "2023-06-02"
output: pdf_document
editor_options: 
  markdown: 
    wrap: sentence
---
# EXERCISE 1 - RISIKO! is back, with friends
```{r}
library(ggplot2)
```


## Point a 
```{r}
games <- readRDS("games_preprocessed.RDS")
```
The dataset *game* contains data about around 25000 board games.The variables that describe each game are eight. They are the year in which it has been published (*yearpublished*), the minimum and the maximum number of players that can play the game (*minplayers*, *maxplayers*) and the minimun age to play (*minage*). Then there is the *average_rating*, *total_owners* and *average_weight*.
```{r}
ggplot(games, aes(x = total_owners, y = average_rating)) +
  geom_point() +
  xlab("Total Owners") +
  ylab("Average Rating") +
  ggtitle("Relationship Between Owners and Ratings")
```
Considering, for example, the two variable *average_rating* and *total_owners*, the plot above make us see which is the relationship between them.

##Point b
```{r}
games <- games[,-c(1,2)]
games <- scale(games)
```

```{r}
cov_matrix <- cov(games)
heatmap(cov_matrix, margins = c(9,9))
cov_matrix
```
From the plot and the variance-covariance matrix values we can see that the variables don't have a strong relationship in general.
```{r}
cor_matrix <- cor(games)
plot(cov_matrix, cor_matrix, 
     xlab = "Variance-Covariance Matrix",
     ylab = "Correlation Matrix")
abline(0,1,col = "red")
```
The correlation matrix is the same as the variance-covariance matrix because our dataset has been scaled, resulting in equivalent covariance and correlation values.
## Point c
Now we perform nonparametric bootstrap to obtain standard error estimates for each of the entries of the variance-covariance matrix.
```{r}
set.seed(12345)

B <- 1000
boot <- function(B, games){
  
  results <- matrix(NA, nrow = B, ncol = ncol(games)^2)
  bootstrap <- replicate(B, {
    
    bootSample <- games[sample(nrow(games), replace = T),]
    bootCov <- cov(bootSample)
    i <- seq_len(B)
    results[i,] <- as.vector(bootCov)
  })
  return(bootstrap)
}

boot <- boot(1000,games)

standard_errors <- apply(boot, 1, sd)
standard_errors
```
```{r}
means <- apply(boot, 1, mean)
qnt_CI <- t(apply(boot, 1, function(z) quantile(z, probs = c(0.025,0.975)))
)
clt_CI <- means + c(-1,1)*1.96*standard_errors

rbind(clt_CI, qnt_CI)
```


```{r}
plot_CI <- function(ci, method) {
  plot(1:length(ci), ci[, 1], type = "l", ylim = range(c(qnt_CI, clt_CI)), 
       xlab = "Bootstrap Iterations", ylab = "Estimate", main = method)
  lines(1:length(ci), ci[, 2], type = "l", col = "red")
  legend("topright", legend = c("Lower CI", "Upper CI"), col = c("black", "red"), lty = 1)
}

# Plot quantile-based confidence intervals
plot_CI(qnt_CI, "Quantile-Based CI")

# Plot CLT-based confidence intervals
plot_CI(clt_CI, "CLT-Based CI")
```
????????????

```{r}
# Plot histogram of bootstrap estimates
hist(means, breaks = "FD", xlab = "Estimate", ylab = "Frequency", 
     main = "Distribution of Bootstrap Estimates")
```
```{r}
plot(1:length(standard_errors), standard_errors, type = "l", 
     xlab = "Bootstrap Iterations", ylab = "Standard Error", 
     main = "Standard Errors of Bootstrap Estimates")
```

## Point d

```{r}
eigen_values <- eigen(cov_matrix)$values
theta <- cumsum(eigen_values) / sum(eigen_values)
theta
```
```{r}
jstar <- min(which(theta > 0.76))
jstar
```

## Point e
```{r}
boot_theta <- matrix(0, nrow = B, ncol = length(theta))
boot_jstar <- numeric(B)

for (i in 1:B) {
  # Generate bootstrap sample by resampling from the original df
  boot_sample <- games[sample(nrow(games), replace= TRUE),]
  
  # Calculate covariance matrix and eigenvalues for the bootstrap
  cov_boot <- cov(boot_sample)
  eigen_boot <- eigen(cov_boot)$values
  
  #Compute theta for the bootstrap sample
  boot_theta[i,] <- cumsum(eigen_boot) / sum(eigen_boot)
  
  #Compute jstar for the bootstrap sample
  boot_jstar[i] <- min(which(boot_theta[i,] > 0.76))
  
}
```

```{r}
bias <- colMeans(boot_theta)-theta
se <- apply(boot_theta, 2, sd)
bias
se
```
```{r}
bias_jstar <- mean(boot_jstar) - jstar
se_jstar <- sd(boot_jstar)
bias_jstar
se_jstar
```
```{r}
p_jstar <- mean(boot_jstar==5)
p_jstar
```
## Point f
```{r}
set.seed(abs(636-555-3226))
ind <- sample(1:nrow(games),5000,FALSE)
sub_data <- games[ind,]

sub_data <- as.data.frame(sub_data)
regression <- lm(sub_data$average_rating~., data = sub_data)
summary(regression)  
```
The intercept is equal to 0.008392. It corresponds to the expected average rate of a board game when the other variables are equal to zero. And so it is not interpretable in this problem because we are talking about the average rating of a board game that does not exist.   
## Point g
Above we used the paired bootstrap method to provide bias, standard error and confidence intervals for regression coefficients, adjusted $R^2$ index and our $\hat{\theta}$. In particular, we chose this method because __________ .
```{r}
num_coeffs <- length(coef(regression))
bootstrap_estimates <- matrix(0, nrow = B, ncol = num_coeffs)


for (i in 1:B) {
  bootstrap_indices <- sample(nrow(sub_data), replace = TRUE)
  
  bootstrap_sample <- sub_data[bootstrap_indices, ]
  
  bootstrap_regression <- lm(average_rating ~ ., data = bootstrap_sample)
  
  bootstrap_estimates[i, ] <- coef(bootstrap_regression)
}

coefficient_bias <- apply(bootstrap_estimates, 2, mean) - coef(regression)
coefficient_bias
```
```{r}
coefficient_std_error <- apply(bootstrap_estimates, 2, sd)
coefficient_std_error
```
```{r}
coefficient_confidence_intervals <- t(apply(bootstrap_estimates, 2, function(x) quantile(x, c(0.025, 0.975))))
coefficient_confidence_intervals
```
```{r}
adjusted_r_squared <- numeric(B)
for (i in 1:B) {
  
  bootstrap_indices <- sample(nrow(sub_data), replace = TRUE)
  
  bootstrap_sample <- sub_data[bootstrap_indices, ]
  
  bootstrap_regression <- lm(average_rating ~ ., data = bootstrap_sample)
  
  adjusted_r_squared[i] <- 1 - (1 - summary(bootstrap_regression)$r.squared) * ((nrow(bootstrap_sample) - 1) / (nrow(bootstrap_sample) - length(coef(bootstrap_regression)) - 1))
}
adjusted_r_squared_bias <- mean(adjusted_r_squared) - summary(regression)$adj.r.squared
adjusted_r_squared_bias
```
```{r}
adjusted_r_squared_std_error <- sd(adjusted_r_squared)
adjusted_r_squared_std_error
```
```{r}
adjusted_r_squared_confidence_interval <- quantile(adjusted_r_squared, c(0.025, 0.975))
adjusted_r_squared_confidence_interval
```
```{r}
for (i in 1:B) {
  bootstrap_indices <- sample(nrow(sub_data), replace = TRUE)
  
  bootstrap_sample <- sub_data[bootstrap_indices, ]
  
  bootstrap_regression <- lm(average_rating ~ ., data = bootstrap_sample)
  
  bootstrap_estimates[i, ] <- coef(bootstrap_regression)
}
```

```{r}
coeff <- coef(regression)
theta_hat <- apply(bootstrap_estimates, 2, function(x) max(((coeff[5]-coeff[6])/(coeff[3]+coeff[2])), 0))
theta_hat_CI <- quantile(theta_hat, c(0.025, 0.975))
theta_hat_CI
```

## Point h
```{r}
# Jackknife approach
num_coeffs <- length(coef(regression))
jackknife_estimates <- matrix(0, nrow = nrow(sub_data), ncol = num_coeffs)

for (i in 1:nrow(sub_data)) {
  # Create jackknife sample by excluding one observation
  jackknife_sample <- sub_data[-i, ]
  
  # Fit the regression model on the jackknife sample
  jackknife_regression <- lm(average_rating ~ ., data = jackknife_sample)
  
  # Store the coefficients of the jackknife regression model
  jackknife_estimates[i, ] <- coef(jackknife_regression)
}

coefficient_bias_jack <- rowMeans(jackknife_estimates) - coef(regression)
coefficient_bias_jack
coefficient_std_error_jack <- sqrt(((nrow(sub_data) - 1) / nrow(sub_data)) * apply((jackknife_estimates - rowMeans(jackknife_estimates))^2, 2, sum))
coefficient_std_error_jack
coefficient_confidence_intervals_jack <- t(apply(jackknife_estimates, 2, function(x) {
  mean_x <- mean(x)
  se_x <- sqrt(((nrow(sub_data) - 1) / nrow(sub_data)) * sum((x - mean_x)^2))
  ci_lower <- mean_x - 1.96 * se_x
  ci_upper <- mean_x + 1.96 * se_x
  c(ci_lower, ci_upper)
}))
coefficient_confidence_intervals_jack

# Calculate adjusted R-squared using the jackknife approach
adjusted_r_squared_jack <- numeric(nrow(sub_data))

for (i in 1:nrow(sub_data)) {
  # Create jackknife sample by excluding one observation
  jackknife_sample <- sub_data[-i, ]
  
  # Fit the regression model on the jackknife sample
  jackknife_regression <- lm(average_rating ~ ., data = jackknife_sample)
  
  # Calculate the adjusted R-squared for the jackknife regression model
  adjusted_r_squared_jack[i] <- 1 - (1 - summary(jackknife_regression)$r.squared) * ((nrow(sub_data) - 1) / (nrow(sub_data) - length(coef(jackknife_regression)) - 1))
}

adjusted_r_squared_CI_jack <- quantile(adjusted_r_squared_jack, c(0.025, 0.975))

adjusted_r_squared_CI_jack

# Calculate theta_hat using the jackknife approach
coeff <- coef(regression)
theta_hat_jack <- apply(jackknife_estimates, 1, function(x) max(((coeff[5] - coeff[6]) / (coeff[3] + coeff[2])), 0))

theta_hat_CI_jack <- quantile(theta_hat_jack, c(0.025, 0.975))
theta_hat_CI_jack

```



# EXERCISE 2- WE NEED SOME MUSIC!

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}

library(boot)
library(tidyverse)
library(reshape2)
library(combinat)

```

DESCRIPTION OF THE VARIABLES AND EXPLORATORY ANALYSIS: Considering the data saved in spot.RDS, a description of the variables can be found on Kaggle:

\- Acousticness: a confidence measure from 0.0 to 1.0 of whether the track is acoustic.
1.0 represents high confidence the track is acoustic.

\- Danceability: danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.
A value of 0.0 is least danceable and 1.0 is most danceable.

\- Duration_ms: the track length in milliseconds.

\- Energy:energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity.
Typically, energetic tracks feel fast, loud, and noisy.

\- Explicit: whether or not the track has explicit lyrics (true = yes it does; false = no it does not OR unknown)

\- id: the Spotify ID for the track.

\- Instrumentalness: predicts whether a track contains no vocals.
"Ooh" and "aah" sounds are treated as instrumental in this context.
Rap or spoken word tracks are clearly "vocal".
The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.

\- Liveness: detects the presence of an audience in the recording.
Higher liveness values represent an increased probability that the track was performed live.
A value above 0.8 provides strong likelihood that the track is live.

\- Loudness: the overall loudness of a track in decibels (dB).

\- Name:name of the track.

\- Popularity: the popularity of a track is a value between 0 and 100, with 100 being the most popular.
The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.
Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.
Duplicate tracks (e.g. the same track from a single and an album) are rated independently.
Artist and album popularity is derived mathematically from track popularity.

\- Release_date of the song

\- Speechiness: speechiness detects the presence of spoken words in a track.
The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.Values below 0.33 most likely represent music and other non-speech-like tracks.

\- Tempo:the overall estimated tempo of a track in beats per minute (BPM).
In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.

\- Valence: a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track.
Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

\- First_auth: first author of the song.

\- n: number of song for each artist.

\- pop: variable containing the original column popularity rescaled between 0 and 1.

```{r}
songs <- readRDS("spot (1).RDS")
head(songs)
```

```{r}
boxplot(popularity~first_auth,data=songs)
```

Focusing on "pop" and given two artists, we want to measure their different in popularity.
Let X and Y denote the vectors of popularity measurements of the songs of two given artists.
We measure their difference with:

$$
T(X, Y) = \left| \text{median}(\text{logit}(X)) - \text{median}(\text{logit}(Y)) \right|
$$ After implementing this estimator to compare the popularity of AC/DC and Kanye West, we provide a a matrix with all pairwise comparisons.

```{r}
u_art<- unique(songs$first_auth)


X <- songs$pop[songs$first_auth == "AC/DC"]

# Extract popularity measurements for Artist Y
Y <- songs$pop[songs$first_auth == "Kanye West"]

med_logit_X <-median( logit(X))
med_logit_Y <-median( logit(Y))

T_XY <- abs(med_logit_X-med_logit_Y)

pop_diff_T <- function(){
  res<-sapply(1:40, function(x) {
    sapply(1:40, function(y) {
      abs(median(logit(songs$pop[songs$first_auth == u_art[x]]))-median(logit(songs$pop[songs$first_auth ==  u_art[y]])))
    })
  })
  return(res)
}

pop_diff<- pop_diff_T()

rownames(pop_diff) <- u_art
colnames(pop_diff) <- u_art

ggplot(melt(pop_diff),aes(Var1,Var2))+
  geom_tile(aes(fill = value), colour = "white")  +
  scale_fill_gradient(low = "white", high = "red")+
  theme(axis.text.x = element_text(angle=90,hjust = 1,siz=5),
        axis.text.y = element_text(hjust = 1,siz=5))

```

Now we perform a hypothesis testing to assess the absence of difference between the popularity of two artists measured via T(X,Y) and perform a sequence of pairwise permutation tests reporting the estimated p-values.

```{r}
data<-songs
p_values <- matrix(0,nrow = length(u_art),ncol = length(u_art))
for (i in 1:length(u_art)){
  for(j in 1:length(u_art)){
   if(i>=j){
    
      X <- data[data$first_auth ==  u_art[i],"pop"]
      Y <- data[data$first_auth ==  u_art[j],"pop"]
      observed_stat=abs(median(logit(songs$pop[songs$first_auth == u_art[i]]))-median(logit(songs$pop[songs$first_auth ==  u_art[j]])))
      
      
      
      num_perm<- 1000
      permutation_stat<- numeric(num_perm)
      for (k in 1:num_perm){
        combined<- union_all(X,Y)
        combined$pop <- sample(combined$pop,nrow(combined),replace = F)
        perm_X<- combined$pop[1:length(X$pop)]
        perm_Y<-combined$pop[(length(X$pop)+1):(length(X$pop)+length(Y$pop))]
        permutation_stat[k]<- abs(median(logit(perm_X))-median(logit(perm_Y)))
        
      }
      
      p_value <- mean(permutation_stat>=observed_stat)
      
      p_values[i,j]<- p_value
      p_values[j,i]<- p_value
      
      
    }
  }
}


p_values

rownames(p_values) <- u_art
colnames(p_values) <- u_art

ggplot(melt(p_values),aes(Var1,Var2))+
  geom_tile(aes(fill = value), colour = "white")  +
  scale_fill_gradient(low = "white", high = "red")+
  theme(axis.text.x = element_text(angle=90,hjust = 1,siz=5),
        axis.text.y = element_text(hjust = 1,siz=5))

```

????????????????????????????????????????????

```{r}
boxplot( data$pop ~ data$first_auth,
         col= cutree( hclust(as.dist(1-p_values)),5))
```

Now, we apply the scale() function to "pop" and perform, for each artist, a single-sample t-test to assess:

$$
H_0: \mu = 0
$$

$$
H_1: \mu > 0
$$

```{r}

pop_scaled <- scale(data$pop)
df<- data.frame(Artist = data$first_auth,pop_scaled)

t_test<- function(df,artist){
  x<- df[df$Artist==artist,"pop_scaled"]
  t_test<- t.test(x,mu=0,alternative = "greater")
  return(t_test$p.value)
}

p_values<- sapply(unique(df$Artist),t_test,df=df)

prob_violation <- sum(p_values<0.10)/length(p_values)
prob_violation #probability of the global null to be violated

plot(sort(p_values), type="l",col = "blue", main = "Density Plot of P-values", xlab = "P-values", ylab = "Density", ylim=c(0,2))
```

The p.adjust function is used to adjust p-values for multiple comparisons.
When conducting multiple statistical tests or comparing multiple groups, the probability of obtaining a significant result by chance increases.
Multiple comparison procedures, such as adjusting p-values, are used to control for this increased risk of false positives (Type I errors).The p.adjust function takes a vector of p-values as input and applies various methods to adjust these p-values: Bonferroni adjustment,Benjamini-Hochberg and Holm.
In summary, the p.adjust function helps in minimizing Type I errors by applying appropriate adjustments to p-values, depending on the chosen method.

```{r}
adjusted_bh <- p.adjust(p_values, method = "BH")
adjusted_bonferroni <- p.adjust(p_values, method = "bonferroni")
adjusted_holm <- p.adjust(p_values, method = "holm")
results <- data.frame(p_values, adjusted_bh, adjusted_bonferroni,adjusted_holm)

# Plotting the p-values and adjusted p-values
plot(sort(p_values), type="l", col = "blue", xlab = "Observation", ylab = "P-value",
     main = "Comparison of P-values and Adjusted P-values")
lines(sort(adjusted_bh), col = "red")
lines(sort(adjusted_bonferroni),  col = "yellow")
lines(sort(adjusted_holm), col = "green")
abline(h = 0.05, lwd = 1, lty = 2)
legend("bottomright", legend = c("P-values", "Adjusted (BH)", "Adjusted (Bonferroni)","Adjusted (Holm)"),
       col = c("blue", "red","yellow","green"), lwd=1,)

```

## HOLM'S PROCEDURE

When the n hypotheses are tested seperately using tests with the level $$\frac{\alpha}{n}$$ it follows immediately from the Boole inequality that the probability of rejecting any true hypothesis is smaller then or equal to alpha. This constitutes a multiple test procedure with the multiple level of signficance alpha for free combination, the classical Bonferroni mupltiple test procedure. 

As written in the paper "A simple sequentially rejective multiple test procedure" of Sture Holm, the sequentially rejective Bonferroni test the obtained levels are compared to the numbers $$\frac{\alpha}{n}, \frac{\alpha}{n-1}, \ldots, \frac{\alpha}{1}$$, whereas in the classical Bonferroni test they are compared to $$\frac{\alpha}{n}$$. 
This means that the probability of rejecting any set of (false) hypotheses using the CLASSICAL Bonferroni test is SMALLER than or equal to the same probability using the sequantially rejective Bonferroni test based on the same test statistics. So the classical Bonferroni tesst can be replaced by the corresponding sequentially rejective Bonferroni test without loosing any probability of rejecting false hypotheses. 
The power gain obtained by using a sequentially rejective Bonferroni test dependes veru much upon the alternative. The power of a hypothesis test is the probability of rejecting the null hypothesis when the alternative hypothesis is the hypothesis that is true. As a consequence, the power gain is small if all the hypothese are "almost true". It may be considerable if a number of hypotheses are "completely wrong". 
